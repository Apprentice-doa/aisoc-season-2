{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dbcefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 8 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   Square_Footage        1000 non-null   int64  \n",
      " 1   Num_Bedrooms          1000 non-null   int64  \n",
      " 2   Num_Bathrooms         1000 non-null   int64  \n",
      " 3   Year_Built            1000 non-null   int64  \n",
      " 4   Lot_Size              1000 non-null   float64\n",
      " 5   Garage_Size           1000 non-null   int64  \n",
      " 6   Neighborhood_Quality  1000 non-null   int64  \n",
      " 7   House_Price           1000 non-null   float64\n",
      "dtypes: float64(2), int64(6)\n",
      "memory usage: 62.6 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('house_price_regression_dataset.csv')\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f723850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Square_Footage</th>\n",
       "      <th>Num_Bedrooms</th>\n",
       "      <th>Num_Bathrooms</th>\n",
       "      <th>Year_Built</th>\n",
       "      <th>Lot_Size</th>\n",
       "      <th>Garage_Size</th>\n",
       "      <th>Neighborhood_Quality</th>\n",
       "      <th>House_Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1360</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1981</td>\n",
       "      <td>0.599637</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.623829e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4272</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2016</td>\n",
       "      <td>4.753014</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>9.852609e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3592</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2016</td>\n",
       "      <td>3.634823</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>7.779774e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>966</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1977</td>\n",
       "      <td>2.730667</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2.296989e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4926</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1993</td>\n",
       "      <td>4.699073</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1.041741e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>3261</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1978</td>\n",
       "      <td>2.165110</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>7.014940e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>3179</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1999</td>\n",
       "      <td>2.977123</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>6.837232e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>2606</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1962</td>\n",
       "      <td>4.055067</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.720240e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>4723</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1950</td>\n",
       "      <td>1.930921</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>9.648653e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>3268</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1983</td>\n",
       "      <td>3.108790</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7.425993e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Square_Footage  Num_Bedrooms  Num_Bathrooms  Year_Built  Lot_Size  \\\n",
       "0              1360             2              1        1981  0.599637   \n",
       "1              4272             3              3        2016  4.753014   \n",
       "2              3592             1              2        2016  3.634823   \n",
       "3               966             1              2        1977  2.730667   \n",
       "4              4926             2              1        1993  4.699073   \n",
       "..              ...           ...            ...         ...       ...   \n",
       "995            3261             4              1        1978  2.165110   \n",
       "996            3179             1              2        1999  2.977123   \n",
       "997            2606             4              2        1962  4.055067   \n",
       "998            4723             5              2        1950  1.930921   \n",
       "999            3268             4              2        1983  3.108790   \n",
       "\n",
       "     Garage_Size  Neighborhood_Quality   House_Price  \n",
       "0              0                     5  2.623829e+05  \n",
       "1              1                     6  9.852609e+05  \n",
       "2              0                     9  7.779774e+05  \n",
       "3              1                     8  2.296989e+05  \n",
       "4              0                     8  1.041741e+06  \n",
       "..           ...                   ...           ...  \n",
       "995            2                    10  7.014940e+05  \n",
       "996            1                    10  6.837232e+05  \n",
       "997            0                     2  5.720240e+05  \n",
       "998            0                     7  9.648653e+05  \n",
       "999            2                     2  7.425993e+05  \n",
       "\n",
       "[1000 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ccb53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "X = data.drop('House_Price', axis=1)\n",
    "y = data['House_Price']\n",
    "\n",
    "# Scaling the features for better performance of the neural network\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69fc9a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.15980323, -0.69383623, -1.18669921, ..., -1.67927849,\n",
       "        -1.25465753, -0.21312613],\n",
       "       [ 1.16072443,  0.00700845,  1.25255918, ...,  1.52238989,\n",
       "        -0.02700828,  0.13342042],\n",
       "       [ 0.61884297, -1.39468091,  0.03292999, ...,  0.66042215,\n",
       "        -1.25465753,  1.17306009],\n",
       "       ...,\n",
       "       [-0.16688515,  0.70785312,  0.03292999, ...,  0.98437109,\n",
       "        -1.25465753, -1.25276579],\n",
       "       [ 1.52011933,  1.4086978 ,  0.03292999, ..., -0.65304553,\n",
       "        -1.25465753,  0.47996698],\n",
       "       [ 0.36065239,  0.70785312,  0.03292999, ...,  0.25492526,\n",
       "         1.20064096, -1.25276579]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da31c6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.25, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad311471",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Oracle/Downloads/aisoc-data-prep-class/.venv/lib/python3.11/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for building the neural network\n",
    "from tensorflow.keras.models import Sequential # type: ignore\n",
    "from tensorflow.keras.layers import Dense, Dropout # type: ignore\n",
    "from tensorflow.keras.optimizers import Adam # pyright: ignore[reportMissingImports]\n",
    "from tensorflow.keras.callbacks import EarlyStopping # type: ignore\n",
    "\n",
    "def create_model(features):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_dim=features), # Input layer\n",
    "        #64 neurons in the first layer, relu for my activation function, features is the number of input features\n",
    "        # (input x  number of neurons) + bias\n",
    "        # (10 x 64) + 64 = 704 parameters\n",
    "        Dropout(0.3),\n",
    "        # Input into the second layer is number of output from the first layer\n",
    "        Dense(32, activation='relu'), #64 inputs, 32 neurons which is 32 output.. # First Hidden layer\n",
    "        # (64 x 32) + 32 = 2080 parameters\n",
    "        Dropout(0.2),\n",
    "        Dense(16, activation='relu'), # Second Hidden layer\n",
    "        # (32 x 16) + 16 = 528 parameters\n",
    "        Dense(1, activation='linear')\n",
    "        # output layer = 1\n",
    "        # input layer = 16\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = create_model(X_train.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e705fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m512\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,137</span> (12.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,137\u001b[0m (12.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,137</span> (12.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,137\u001b[0m (12.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compile the model including the optimiser and loss function\n",
    "# Adam optimizer is a popular choice for regression tasks\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6f2916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 10228658176.0000 - mae: 79211.1562 - val_loss: 1501828352.0000 - val_mae: 30678.1191\n",
      "Epoch 2/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9468955648.0000 - mae: 77379.3672 - val_loss: 1506825728.0000 - val_mae: 30669.9609\n",
      "Epoch 3/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9260590080.0000 - mae: 74598.7500 - val_loss: 1508766336.0000 - val_mae: 30700.6172\n",
      "Epoch 4/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9521357824.0000 - mae: 77460.2969 - val_loss: 1497905280.0000 - val_mae: 30604.2461\n",
      "Epoch 5/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10221686784.0000 - mae: 79242.9297 - val_loss: 1564284672.0000 - val_mae: 31287.3027\n",
      "Epoch 6/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9277577216.0000 - mae: 75980.1641 - val_loss: 1526207360.0000 - val_mae: 30969.5293\n",
      "Epoch 7/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9669450752.0000 - mae: 76933.5469 - val_loss: 1443602048.0000 - val_mae: 30144.0938\n",
      "Epoch 8/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10176890880.0000 - mae: 79328.2734 - val_loss: 1492825088.0000 - val_mae: 30626.1660\n",
      "Epoch 9/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9387719680.0000 - mae: 75810.6875 - val_loss: 1489076608.0000 - val_mae: 30711.9297\n",
      "Epoch 10/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10065300480.0000 - mae: 78269.3438 - val_loss: 1472552064.0000 - val_mae: 30466.0840\n",
      "Epoch 11/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8464595456.0000 - mae: 72642.3047 - val_loss: 1402168960.0000 - val_mae: 29859.8867\n",
      "Epoch 12/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9349561344.0000 - mae: 76521.0000 - val_loss: 1433985152.0000 - val_mae: 30112.6094\n",
      "Epoch 13/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9418929152.0000 - mae: 76597.9609 - val_loss: 1452185088.0000 - val_mae: 30296.9004\n",
      "Epoch 14/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9745697792.0000 - mae: 78190.1562 - val_loss: 1440679296.0000 - val_mae: 30148.4902\n",
      "Epoch 15/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10497145856.0000 - mae: 81050.4922 - val_loss: 1501617280.0000 - val_mae: 30917.5293\n",
      "Epoch 16/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9640707072.0000 - mae: 76697.5078 - val_loss: 1503819904.0000 - val_mae: 30844.8340\n",
      "Epoch 17/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10620974080.0000 - mae: 79451.4297 - val_loss: 1440176256.0000 - val_mae: 30044.4707\n",
      "Epoch 18/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9592820736.0000 - mae: 76276.2344 - val_loss: 1410325376.0000 - val_mae: 29764.7598\n",
      "Epoch 19/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10194486272.0000 - mae: 77469.9453 - val_loss: 1371972992.0000 - val_mae: 29469.9395\n",
      "Epoch 20/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9203864576.0000 - mae: 74520.7969 - val_loss: 1334287232.0000 - val_mae: 29179.2109\n",
      "Epoch 21/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10102637568.0000 - mae: 79757.6875 - val_loss: 1293696384.0000 - val_mae: 28917.2129\n",
      "Epoch 22/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9522860032.0000 - mae: 77443.7109 - val_loss: 1349495680.0000 - val_mae: 29366.3770\n",
      "Epoch 23/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9885821952.0000 - mae: 78096.7969 - val_loss: 1380748032.0000 - val_mae: 29704.6309\n",
      "Epoch 24/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9459753984.0000 - mae: 76881.1484 - val_loss: 1328902784.0000 - val_mae: 29316.7324\n",
      "Epoch 25/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8706353152.0000 - mae: 73584.6953 - val_loss: 1342843392.0000 - val_mae: 29470.0957\n",
      "Epoch 26/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8873767936.0000 - mae: 73838.7344 - val_loss: 1370771968.0000 - val_mae: 29679.6641\n",
      "Epoch 27/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9431702528.0000 - mae: 75714.4453 - val_loss: 1455063424.0000 - val_mae: 30479.1426\n",
      "Epoch 28/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10770693120.0000 - mae: 80937.9297 - val_loss: 1411126272.0000 - val_mae: 29990.8027\n",
      "Epoch 29/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9950377984.0000 - mae: 79255.0703 - val_loss: 1344614528.0000 - val_mae: 29305.8672\n",
      "Epoch 30/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9948350464.0000 - mae: 77562.9297 - val_loss: 1292721408.0000 - val_mae: 28896.1738\n",
      "Epoch 31/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9577136128.0000 - mae: 76215.8125 - val_loss: 1284412032.0000 - val_mae: 28769.1309\n",
      "Epoch 32/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9641125888.0000 - mae: 76329.7969 - val_loss: 1286110336.0000 - val_mae: 28714.8203\n",
      "Epoch 33/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10281948160.0000 - mae: 78815.9844 - val_loss: 1325612672.0000 - val_mae: 29104.8008\n",
      "Epoch 34/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9381602304.0000 - mae: 76087.2812 - val_loss: 1320123648.0000 - val_mae: 29079.1191\n",
      "Epoch 35/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9356740608.0000 - mae: 75595.3984 - val_loss: 1335968896.0000 - val_mae: 29264.5273\n",
      "Epoch 36/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9209877504.0000 - mae: 75469.2422 - val_loss: 1375073024.0000 - val_mae: 29696.3027\n",
      "Epoch 37/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9741448192.0000 - mae: 78530.7109 - val_loss: 1471569536.0000 - val_mae: 30855.6426\n",
      "Epoch 38/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10063311872.0000 - mae: 79634.6406 - val_loss: 1345252096.0000 - val_mae: 29323.5664\n",
      "Epoch 39/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10473150464.0000 - mae: 79051.3984 - val_loss: 1303984512.0000 - val_mae: 28978.1973\n",
      "Epoch 40/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9559860224.0000 - mae: 76081.4688 - val_loss: 1304427264.0000 - val_mae: 28977.8164\n",
      "Epoch 41/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9795840000.0000 - mae: 76187.8438 - val_loss: 1278858624.0000 - val_mae: 28827.0898\n",
      "Epoch 42/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8408946688.0000 - mae: 70874.9297 - val_loss: 1287599616.0000 - val_mae: 28902.6328\n",
      "Epoch 43/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8235152384.0000 - mae: 72174.0234 - val_loss: 1227923712.0000 - val_mae: 28183.8008\n",
      "Epoch 44/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8523902464.0000 - mae: 72047.2578 - val_loss: 1198478464.0000 - val_mae: 27881.1992\n",
      "Epoch 45/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8576306688.0000 - mae: 71116.1406 - val_loss: 1179196928.0000 - val_mae: 27817.5117\n",
      "Epoch 46/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9811461120.0000 - mae: 77327.4766 - val_loss: 1195326464.0000 - val_mae: 27922.1465\n",
      "Epoch 47/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10325646336.0000 - mae: 79460.7266 - val_loss: 1212575616.0000 - val_mae: 28101.3398\n",
      "Epoch 48/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9747869696.0000 - mae: 76300.1719 - val_loss: 1236272256.0000 - val_mae: 28294.5371\n",
      "Epoch 49/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9124950016.0000 - mae: 75344.6562 - val_loss: 1242664448.0000 - val_mae: 28194.8008\n",
      "Epoch 50/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9447262208.0000 - mae: 74684.5469 - val_loss: 1214542208.0000 - val_mae: 27843.2715\n",
      "Epoch 51/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9683813376.0000 - mae: 76657.6484 - val_loss: 1209169536.0000 - val_mae: 27854.4062\n",
      "Epoch 52/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8763916288.0000 - mae: 74817.5625 - val_loss: 1187643264.0000 - val_mae: 27549.0918\n",
      "Epoch 53/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8888859648.0000 - mae: 74060.1719 - val_loss: 1275718912.0000 - val_mae: 28586.7695\n",
      "Epoch 54/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9641936896.0000 - mae: 77004.8438 - val_loss: 1275224192.0000 - val_mae: 28582.8301\n",
      "Epoch 55/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8849269760.0000 - mae: 73925.4766 - val_loss: 1171096704.0000 - val_mae: 27359.1738\n",
      "Epoch 56/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9410686976.0000 - mae: 75603.4766 - val_loss: 1122992384.0000 - val_mae: 26994.3398\n",
      "Epoch 57/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9491647488.0000 - mae: 76105.9531 - val_loss: 1218296576.0000 - val_mae: 27971.2070\n",
      "Epoch 58/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9624182784.0000 - mae: 76562.6250 - val_loss: 1210375552.0000 - val_mae: 27919.6426\n",
      "Epoch 59/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8092754944.0000 - mae: 70217.3906 - val_loss: 1234324736.0000 - val_mae: 28265.3965\n",
      "Epoch 60/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9247560704.0000 - mae: 75062.3359 - val_loss: 1189146240.0000 - val_mae: 27711.4375\n",
      "Epoch 61/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9060896768.0000 - mae: 73149.2500 - val_loss: 1133589120.0000 - val_mae: 27031.5859\n",
      "Epoch 62/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9142650880.0000 - mae: 75678.7109 - val_loss: 1124002944.0000 - val_mae: 26914.4199\n",
      "Epoch 63/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9175700480.0000 - mae: 75351.3594 - val_loss: 1108835712.0000 - val_mae: 26812.0781\n",
      "Epoch 64/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9830895616.0000 - mae: 77771.0469 - val_loss: 1118548352.0000 - val_mae: 26941.1680\n",
      "Epoch 65/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8672468992.0000 - mae: 73388.5625 - val_loss: 1116212224.0000 - val_mae: 26922.2461\n",
      "Epoch 66/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8628213760.0000 - mae: 75137.6797 - val_loss: 1075623168.0000 - val_mae: 26529.0137\n",
      "Epoch 67/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9672186880.0000 - mae: 77421.1328 - val_loss: 1102874112.0000 - val_mae: 26815.4141\n",
      "Epoch 68/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9380709376.0000 - mae: 75255.1562 - val_loss: 1177221632.0000 - val_mae: 27756.4551\n",
      "Epoch 69/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9415830528.0000 - mae: 75796.0859 - val_loss: 1151866624.0000 - val_mae: 27450.3125\n",
      "Epoch 70/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8753071104.0000 - mae: 71506.4531 - val_loss: 1088090752.0000 - val_mae: 26771.9805\n",
      "Epoch 71/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9140134912.0000 - mae: 73727.7578 - val_loss: 1045623616.0000 - val_mae: 26576.0586\n",
      "Epoch 72/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10014854144.0000 - mae: 78477.6406 - val_loss: 1099344384.0000 - val_mae: 26946.0547\n",
      "Epoch 73/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9386968064.0000 - mae: 76016.0781 - val_loss: 1103110400.0000 - val_mae: 26903.4609\n",
      "Epoch 74/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8863544320.0000 - mae: 73349.2344 - val_loss: 1128307712.0000 - val_mae: 27141.8184\n",
      "Epoch 75/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8973897728.0000 - mae: 74676.2344 - val_loss: 1060465344.0000 - val_mae: 26388.1094\n",
      "Epoch 76/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8942892032.0000 - mae: 73279.4531 - val_loss: 1021531904.0000 - val_mae: 26138.1523\n",
      "Epoch 77/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8617155584.0000 - mae: 72327.8125 - val_loss: 1031803456.0000 - val_mae: 26130.4766\n",
      "Epoch 78/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8763584512.0000 - mae: 72973.3672 - val_loss: 1091475712.0000 - val_mae: 26606.0293\n",
      "Epoch 79/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9670404096.0000 - mae: 75484.0234 - val_loss: 1125389824.0000 - val_mae: 27033.3730\n",
      "Epoch 80/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8786664448.0000 - mae: 73543.8828 - val_loss: 1069197568.0000 - val_mae: 26445.7012\n",
      "Epoch 81/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9519843328.0000 - mae: 75922.6484 - val_loss: 1080211840.0000 - val_mae: 26489.5625\n",
      "Epoch 82/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8819670016.0000 - mae: 73845.4688 - val_loss: 1090209920.0000 - val_mae: 26553.2422\n",
      "Epoch 83/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9362760704.0000 - mae: 76236.1562 - val_loss: 1039910912.0000 - val_mae: 26048.2227\n",
      "Epoch 84/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9109219328.0000 - mae: 74815.2344 - val_loss: 1012466880.0000 - val_mae: 25691.8398\n",
      "Epoch 85/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8573639680.0000 - mae: 72966.3672 - val_loss: 1009426624.0000 - val_mae: 25719.1973\n",
      "Epoch 86/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9969635328.0000 - mae: 75335.2656 - val_loss: 1144550400.0000 - val_mae: 27371.2109\n",
      "Epoch 87/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8492567552.0000 - mae: 71961.5703 - val_loss: 1138682240.0000 - val_mae: 27256.2500\n",
      "Epoch 88/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9501956096.0000 - mae: 77101.0781 - val_loss: 1047600384.0000 - val_mae: 26036.6309\n",
      "Epoch 89/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8457764864.0000 - mae: 72011.7969 - val_loss: 1056986560.0000 - val_mae: 26177.7773\n",
      "Epoch 90/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9025525760.0000 - mae: 74759.5312 - val_loss: 986780672.0000 - val_mae: 25438.0059\n",
      "Epoch 91/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9667450880.0000 - mae: 76652.4375 - val_loss: 1046438656.0000 - val_mae: 26172.8730\n",
      "Epoch 92/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9125906432.0000 - mae: 76500.3516 - val_loss: 1029643840.0000 - val_mae: 25973.3535\n",
      "Epoch 93/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7981488128.0000 - mae: 69478.5547 - val_loss: 1018874112.0000 - val_mae: 25876.6172\n",
      "Epoch 94/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8320157184.0000 - mae: 69492.4922 - val_loss: 1067552832.0000 - val_mae: 26518.3926\n",
      "Epoch 95/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8649545728.0000 - mae: 74037.2578 - val_loss: 989136576.0000 - val_mae: 25515.6172\n",
      "Epoch 96/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8693472256.0000 - mae: 73048.0781 - val_loss: 1002276864.0000 - val_mae: 25661.7207\n",
      "Epoch 97/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8586939392.0000 - mae: 72133.8047 - val_loss: 987560704.0000 - val_mae: 25536.5469\n",
      "Epoch 98/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8900389888.0000 - mae: 73608.2266 - val_loss: 977115520.0000 - val_mae: 25440.9238\n",
      "Epoch 99/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8727740416.0000 - mae: 74091.6250 - val_loss: 1028617344.0000 - val_mae: 25973.4453\n",
      "Epoch 100/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9257469952.0000 - mae: 74903.0625 - val_loss: 975550080.0000 - val_mae: 25341.3242\n",
      "Epoch 101/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9931894784.0000 - mae: 77051.7031 - val_loss: 983615488.0000 - val_mae: 25447.3418\n",
      "Epoch 102/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8745358336.0000 - mae: 71528.6484 - val_loss: 958725248.0000 - val_mae: 25134.4590\n",
      "Epoch 103/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9349329920.0000 - mae: 73279.8594 - val_loss: 997277120.0000 - val_mae: 25557.9629\n",
      "Epoch 104/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9866617856.0000 - mae: 78815.0391 - val_loss: 916397184.0000 - val_mae: 24495.2461\n",
      "Epoch 105/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8176499200.0000 - mae: 68451.8438 - val_loss: 926235392.0000 - val_mae: 24634.0410\n",
      "Epoch 106/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9398450176.0000 - mae: 74707.9375 - val_loss: 954075840.0000 - val_mae: 25000.9160\n",
      "Epoch 107/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8687694848.0000 - mae: 71999.6797 - val_loss: 978078912.0000 - val_mae: 25207.7129\n",
      "Epoch 108/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8832318464.0000 - mae: 72837.3203 - val_loss: 1037928832.0000 - val_mae: 25921.5430\n",
      "Epoch 109/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9550903296.0000 - mae: 76906.8906 - val_loss: 1017251840.0000 - val_mae: 25629.2227\n",
      "Epoch 110/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9233072128.0000 - mae: 73177.3594 - val_loss: 942605376.0000 - val_mae: 24802.6953\n",
      "Epoch 111/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8827404288.0000 - mae: 73733.0078 - val_loss: 972740032.0000 - val_mae: 25188.5039\n",
      "Epoch 112/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9061790720.0000 - mae: 73563.0938 - val_loss: 1057762624.0000 - val_mae: 26340.8574\n",
      "Epoch 113/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9047729152.0000 - mae: 72681.4922 - val_loss: 1157648384.0000 - val_mae: 27872.9141\n",
      "Epoch 114/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7817512448.0000 - mae: 67563.5000 - val_loss: 1067128576.0000 - val_mae: 26648.9238\n",
      "Epoch 115/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8291436544.0000 - mae: 69734.6562 - val_loss: 1108774016.0000 - val_mae: 27312.3086\n",
      "Epoch 116/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9023606784.0000 - mae: 73458.0391 - val_loss: 1056079232.0000 - val_mae: 26607.7832\n",
      "Epoch 117/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8628069376.0000 - mae: 72304.8281 - val_loss: 989756160.0000 - val_mae: 25760.2031\n",
      "Epoch 118/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9520217088.0000 - mae: 76749.9609 - val_loss: 898432896.0000 - val_mae: 24407.1738\n",
      "Epoch 119/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8721866752.0000 - mae: 72848.4766 - val_loss: 900552768.0000 - val_mae: 24435.1836\n",
      "Epoch 120/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8579971584.0000 - mae: 72775.0625 - val_loss: 907293056.0000 - val_mae: 24595.8027\n",
      "Epoch 121/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8821342208.0000 - mae: 73155.5234 - val_loss: 871842432.0000 - val_mae: 24081.5195\n",
      "Epoch 122/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8138473472.0000 - mae: 69444.3828 - val_loss: 939444480.0000 - val_mae: 25110.6543\n",
      "Epoch 123/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8273355264.0000 - mae: 70666.6016 - val_loss: 952150464.0000 - val_mae: 25303.2266\n",
      "Epoch 124/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8067009536.0000 - mae: 69856.0469 - val_loss: 971629952.0000 - val_mae: 25623.3086\n",
      "Epoch 125/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8778123264.0000 - mae: 72351.8438 - val_loss: 898980480.0000 - val_mae: 24551.4805\n",
      "Epoch 126/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8163728896.0000 - mae: 69466.6641 - val_loss: 846121408.0000 - val_mae: 23861.8262\n",
      "Epoch 127/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9116395520.0000 - mae: 73770.6484 - val_loss: 836143232.0000 - val_mae: 23700.7871\n",
      "Epoch 128/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8859157504.0000 - mae: 72220.5938 - val_loss: 781946304.0000 - val_mae: 23056.1875\n",
      "Epoch 129/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8305503232.0000 - mae: 72174.5859 - val_loss: 806015808.0000 - val_mae: 23366.4355\n",
      "Epoch 130/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9128823808.0000 - mae: 72866.9453 - val_loss: 843413056.0000 - val_mae: 23855.1074\n",
      "Epoch 131/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8671529984.0000 - mae: 72484.2500 - val_loss: 861269056.0000 - val_mae: 24096.0996\n",
      "Epoch 132/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7849911808.0000 - mae: 68075.5703 - val_loss: 796024128.0000 - val_mae: 23331.8984\n",
      "Epoch 133/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8327473152.0000 - mae: 70838.8047 - val_loss: 803201792.0000 - val_mae: 23427.4531\n",
      "Epoch 134/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8469227008.0000 - mae: 71452.2656 - val_loss: 824420800.0000 - val_mae: 23710.8613\n",
      "Epoch 135/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7977771520.0000 - mae: 69529.0234 - val_loss: 904550464.0000 - val_mae: 24731.6035\n",
      "Epoch 136/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9118951424.0000 - mae: 73540.9297 - val_loss: 873090496.0000 - val_mae: 24330.5762\n",
      "Epoch 137/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8855819264.0000 - mae: 72746.8438 - val_loss: 997211584.0000 - val_mae: 26073.0117\n",
      "Epoch 138/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8271660544.0000 - mae: 72700.5234 - val_loss: 946462144.0000 - val_mae: 25328.4414\n",
      "Epoch 139/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8822004736.0000 - mae: 69723.8047 - val_loss: 830037568.0000 - val_mae: 23667.1055\n",
      "Epoch 140/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7843186176.0000 - mae: 68535.9297 - val_loss: 820720896.0000 - val_mae: 23495.0527\n",
      "Epoch 141/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8881553408.0000 - mae: 72794.8672 - val_loss: 817838976.0000 - val_mae: 23456.8828\n",
      "Epoch 142/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7977000960.0000 - mae: 68476.6562 - val_loss: 825733056.0000 - val_mae: 23579.8340\n",
      "Epoch 143/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8675054592.0000 - mae: 71562.1328 - val_loss: 911892736.0000 - val_mae: 24893.0430\n",
      "Epoch 144/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8615072768.0000 - mae: 70500.6953 - val_loss: 1057260416.0000 - val_mae: 27112.3809\n",
      "Epoch 145/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8555293184.0000 - mae: 71442.5234 - val_loss: 870445696.0000 - val_mae: 24259.0332\n",
      "Epoch 146/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8283447808.0000 - mae: 71812.3281 - val_loss: 794737408.0000 - val_mae: 23069.8496\n",
      "Epoch 147/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8004784640.0000 - mae: 70294.7344 - val_loss: 797508544.0000 - val_mae: 23126.1465\n",
      "Epoch 148/1000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9006702592.0000 - mae: 73826.2891 - val_loss: 839890368.0000 - val_mae: 23817.6875\n"
     ]
    }
   ],
   "source": [
    "# Early stopping to prevent redundant training\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=20,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "# Using a batch size of 32 or less, depending on the size of the training set\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=1000, \n",
    "    batch_size=min(32, len(X_train)//4),\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80652a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Testing RMSE: 30173.7528\n"
     ]
    }
   ],
   "source": [
    "#y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "#train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "#print(f\"Training RMSE: {train_rmse:.4f}\")\n",
    "print(f\"Testing RMSE: {test_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d86c156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00d407e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc623bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
